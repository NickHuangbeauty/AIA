{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import walk\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat = ['bedroom', 'CALsuburb', 'coast', 'forest', 'highway', 'industrial', 'insidecity', 'kitchen', 'livingroom', 'mountain', 'opencountry', 'PARoffice', 'store', 'street', 'tallbuilding']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = len(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath=\"/data/examples/may_the_4_be_with_u/where_am_i/train\"\n",
    "testpath=\"/data/examples/may_the_4_be_with_u/where_am_i\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "141\n",
      "260\n",
      "228\n",
      "160\n",
      "211\n",
      "208\n",
      "110\n",
      "189\n",
      "274\n",
      "310\n",
      "115\n",
      "215\n",
      "192\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "img_total = []\n",
    "label = []\n",
    "for i in range(len(cat)):\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in walk(trainpath +'/{}'.format(cat[i])):\n",
    "        f.extend(filenames) \n",
    "    print(len(f))\n",
    "    for j in range(len(f)):\n",
    "        imgp = trainpath +'/{}/{}'.format(cat[i], f[j])\n",
    "        img = image.load_img(imgp, target_size=(224, 224), grayscale = True)\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = img/255\n",
    "        img_total.append(img)\n",
    "        label.append(cat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = np.array(img_total)\n",
    "img_train = img_train.reshape(2985, 224, 224, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = img_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(label)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(label)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y_TrainOneHot = onehot_encoder.fit_transform(integer_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (5, 5), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (5, 5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (5, 5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "#opt = keras.optimizers.Adam()\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph2', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(img_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "93/93 [==============================] - 32s 347ms/step - loss: 2.6535 - acc: 0.1077\n",
      "Epoch 2/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 2.4096 - acc: 0.1981\n",
      "Epoch 3/120\n",
      "93/93 [==============================] - 27s 296ms/step - loss: 2.2749 - acc: 0.2610\n",
      "Epoch 4/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 2.1631 - acc: 0.2849\n",
      "Epoch 5/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 1.9958 - acc: 0.3528\n",
      "Epoch 6/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 1.8278 - acc: 0.4048\n",
      "Epoch 7/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 1.7066 - acc: 0.4271\n",
      "Epoch 8/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 1.6406 - acc: 0.4502\n",
      "Epoch 9/120\n",
      "93/93 [==============================] - 27s 294ms/step - loss: 1.5704 - acc: 0.4803\n",
      "Epoch 10/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 1.5020 - acc: 0.4947\n",
      "Epoch 11/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 1.4921 - acc: 0.4965\n",
      "Epoch 12/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 1.4388 - acc: 0.5157\n",
      "Epoch 13/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 1.3826 - acc: 0.5322\n",
      "Epoch 14/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 1.3509 - acc: 0.5411\n",
      "Epoch 15/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 1.2982 - acc: 0.5601\n",
      "Epoch 16/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 1.2939 - acc: 0.5547\n",
      "Epoch 17/120\n",
      "93/93 [==============================] - 28s 299ms/step - loss: 1.2215 - acc: 0.5733\n",
      "Epoch 18/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 1.2196 - acc: 0.5854\n",
      "Epoch 19/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 1.2061 - acc: 0.5997\n",
      "Epoch 20/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 1.1870 - acc: 0.6008\n",
      "Epoch 21/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 1.1341 - acc: 0.6122\n",
      "Epoch 22/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 1.1232 - acc: 0.6125\n",
      "Epoch 23/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 1.1396 - acc: 0.6059\n",
      "Epoch 24/120\n",
      "93/93 [==============================] - 28s 299ms/step - loss: 1.1141 - acc: 0.6216\n",
      "Epoch 25/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 1.0958 - acc: 0.6252\n",
      "Epoch 26/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 1.0690 - acc: 0.6368\n",
      "Epoch 27/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 1.0104 - acc: 0.6517\n",
      "Epoch 28/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 1.0616 - acc: 0.6266\n",
      "Epoch 29/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 1.0703 - acc: 0.6394\n",
      "Epoch 30/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 1.0096 - acc: 0.6614\n",
      "Epoch 31/120\n",
      "93/93 [==============================] - 27s 294ms/step - loss: 0.9752 - acc: 0.6502\n",
      "Epoch 32/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 1.0109 - acc: 0.6557\n",
      "Epoch 33/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 1.0000 - acc: 0.6603\n",
      "Epoch 34/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.9712 - acc: 0.6713\n",
      "Epoch 35/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.9322 - acc: 0.6784\n",
      "Epoch 36/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.9670 - acc: 0.6680\n",
      "Epoch 37/120\n",
      "93/93 [==============================] - 27s 296ms/step - loss: 0.9534 - acc: 0.6676\n",
      "Epoch 38/120\n",
      "93/93 [==============================] - 27s 294ms/step - loss: 0.8746 - acc: 0.6967\n",
      "Epoch 39/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.9471 - acc: 0.6765\n",
      "Epoch 40/120\n",
      "93/93 [==============================] - 27s 296ms/step - loss: 0.8696 - acc: 0.7086\n",
      "Epoch 41/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.8706 - acc: 0.7006\n",
      "Epoch 42/120\n",
      "93/93 [==============================] - 27s 296ms/step - loss: 0.9198 - acc: 0.6979\n",
      "Epoch 43/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.8791 - acc: 0.6970\n",
      "Epoch 44/120\n",
      "93/93 [==============================] - 27s 294ms/step - loss: 0.8964 - acc: 0.6936\n",
      "Epoch 45/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 0.8633 - acc: 0.6932\n",
      "Epoch 46/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.8486 - acc: 0.7105\n",
      "Epoch 47/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.8434 - acc: 0.7016\n",
      "Epoch 48/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.8159 - acc: 0.7222\n",
      "Epoch 49/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.8274 - acc: 0.7004\n",
      "Epoch 50/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.8313 - acc: 0.7135\n",
      "Epoch 51/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.8478 - acc: 0.7100\n",
      "Epoch 52/120\n",
      "93/93 [==============================] - 27s 294ms/step - loss: 0.7880 - acc: 0.7316\n",
      "Epoch 53/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.8144 - acc: 0.7241\n",
      "Epoch 54/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 0.7878 - acc: 0.7218\n",
      "Epoch 55/120\n",
      "93/93 [==============================] - 27s 294ms/step - loss: 0.7815 - acc: 0.7163\n",
      "Epoch 56/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 0.7836 - acc: 0.7332\n",
      "Epoch 57/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.8371 - acc: 0.7177\n",
      "Epoch 58/120\n",
      "93/93 [==============================] - 27s 294ms/step - loss: 0.8048 - acc: 0.7227\n",
      "Epoch 59/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 0.7607 - acc: 0.7403\n",
      "Epoch 60/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.7490 - acc: 0.7408\n",
      "Epoch 61/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.7819 - acc: 0.7317\n",
      "Epoch 62/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.7629 - acc: 0.7329\n",
      "Epoch 63/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.7465 - acc: 0.7474\n",
      "Epoch 64/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.7323 - acc: 0.7360\n",
      "Epoch 65/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.7333 - acc: 0.7390\n",
      "Epoch 66/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.7310 - acc: 0.7459\n",
      "Epoch 67/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.7332 - acc: 0.7522\n",
      "Epoch 68/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.7211 - acc: 0.7482\n",
      "Epoch 69/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 0.7428 - acc: 0.7560\n",
      "Epoch 70/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.7074 - acc: 0.7617\n",
      "Epoch 71/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.7260 - acc: 0.7518\n",
      "Epoch 72/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 0.6837 - acc: 0.7618\n",
      "Epoch 73/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.6811 - acc: 0.7661\n",
      "Epoch 74/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 0.6843 - acc: 0.7695\n",
      "Epoch 75/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6832 - acc: 0.7676\n",
      "Epoch 76/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6772 - acc: 0.7594\n",
      "Epoch 77/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6821 - acc: 0.7595\n",
      "Epoch 78/120\n",
      "93/93 [==============================] - 27s 293ms/step - loss: 0.6624 - acc: 0.7627\n",
      "Epoch 79/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.6696 - acc: 0.7748\n",
      "Epoch 80/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.6756 - acc: 0.7638\n",
      "Epoch 81/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.7096 - acc: 0.7574\n",
      "Epoch 82/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6602 - acc: 0.7683\n",
      "Epoch 83/120\n",
      "93/93 [==============================] - 27s 296ms/step - loss: 0.6244 - acc: 0.7799\n",
      "Epoch 84/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6701 - acc: 0.7731\n",
      "Epoch 85/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6487 - acc: 0.7762\n",
      "Epoch 86/120\n",
      "93/93 [==============================] - 27s 296ms/step - loss: 0.6400 - acc: 0.7846\n",
      "Epoch 87/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6416 - acc: 0.7743\n",
      "Epoch 88/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6094 - acc: 0.7827\n",
      "Epoch 89/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.6572 - acc: 0.7659\n",
      "Epoch 90/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6299 - acc: 0.7837\n",
      "Epoch 91/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6456 - acc: 0.7800\n",
      "Epoch 92/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6560 - acc: 0.7779\n",
      "Epoch 93/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6644 - acc: 0.7689\n",
      "Epoch 94/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6473 - acc: 0.7718\n",
      "Epoch 95/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6237 - acc: 0.7827\n",
      "Epoch 96/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6090 - acc: 0.7904\n",
      "Epoch 97/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6105 - acc: 0.7851\n",
      "Epoch 98/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6430 - acc: 0.7696\n",
      "Epoch 99/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.6084 - acc: 0.7856\n",
      "Epoch 100/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.5964 - acc: 0.7951\n",
      "Epoch 101/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.5890 - acc: 0.7963\n",
      "Epoch 102/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.6023 - acc: 0.7920\n",
      "Epoch 103/120\n",
      "93/93 [==============================] - 27s 296ms/step - loss: 0.5973 - acc: 0.7950\n",
      "Epoch 104/120\n",
      "93/93 [==============================] - 28s 298ms/step - loss: 0.5893 - acc: 0.7839\n",
      "Epoch 105/120\n",
      "93/93 [==============================] - 27s 294ms/step - loss: 0.5879 - acc: 0.7879\n",
      "Epoch 106/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.5941 - acc: 0.7898\n",
      "Epoch 107/120\n",
      "93/93 [==============================] - 27s 296ms/step - loss: 0.5723 - acc: 0.8022\n",
      "Epoch 108/120\n",
      "93/93 [==============================] - 28s 296ms/step - loss: 0.5810 - acc: 0.7962\n",
      "Epoch 109/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.5544 - acc: 0.8001\n",
      "Epoch 110/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.5595 - acc: 0.8008\n",
      "Epoch 111/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.5538 - acc: 0.7985\n",
      "Epoch 112/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.5516 - acc: 0.8099\n",
      "Epoch 113/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.5503 - acc: 0.7997\n",
      "Epoch 114/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.5426 - acc: 0.8071\n",
      "Epoch 115/120\n",
      "93/93 [==============================] - 27s 296ms/step - loss: 0.5702 - acc: 0.7975\n",
      "Epoch 116/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.5794 - acc: 0.7932\n",
      "Epoch 117/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.5474 - acc: 0.8002\n",
      "Epoch 118/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.5547 - acc: 0.8056\n",
      "Epoch 119/120\n",
      "93/93 [==============================] - 28s 297ms/step - loss: 0.5261 - acc: 0.8276\n",
      "Epoch 120/120\n",
      "93/93 [==============================] - 27s 295ms/step - loss: 0.5226 - acc: 0.8219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8f62092b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_TrainOneHot,batch_size=32),\n",
    "                    steps_per_epoch=x_train.shape[0] // 32,\n",
    "                    epochs=130,\n",
    "                    validation_data=None, \n",
    "                    callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test = []\n",
    "f = []\n",
    "pred = []\n",
    "for (dirpath, dirnames, filenames) in walk(testpath +'/testset'):\n",
    "    f.extend(filenames) \n",
    "for j in range(len(f)):\n",
    "    imgp = testpath + '/testset/{}'.format(f[j])\n",
    "    img = image.load_img(imgp, target_size=(224, 224), grayscale = True)\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img/255\n",
    "    p = model.predict(img)\n",
    "    pred.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [np.argmax(r) for r in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {0:9,\n",
    "      1:7,\n",
    "      2:12,\n",
    "      3:10,\n",
    "      4:4,\n",
    "      5:14,\n",
    "      6:2,\n",
    "      7:3,\n",
    "      8:0,\n",
    "      9:5,\n",
    "      10:8,\n",
    "      11:6,\n",
    "      12:11,\n",
    "      13:1,\n",
    "      14:13}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cla = []\n",
    "for i in range(len(k)):\n",
    "    cla.append(dic[k[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(len(f)):\n",
    "    a.append(f[i][:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['class'] = cla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.columns = ['id','class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7dc74b51e229d841272c0795cffed857d0e6038a4be0c9...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b1940d44fe4f5b76e89f876de3d2514a51b50057cae3e...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7cb3479dac3e7dcc69241f4cd957380a48399b1aa0480d...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dbbcb75061f5a6ac493af0fa87ce806977a0a4cebce9b7...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d3238e3700b076099b4fad94dc1714d2bf14c7aedb2e24...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  class\n",
       "0  7dc74b51e229d841272c0795cffed857d0e6038a4be0c9...      7\n",
       "1  1b1940d44fe4f5b76e89f876de3d2514a51b50057cae3e...      3\n",
       "2  7cb3479dac3e7dcc69241f4cd957380a48399b1aa0480d...      3\n",
       "3  dbbcb75061f5a6ac493af0fa87ce806977a0a4cebce9b7...      3\n",
       "4  d3238e3700b076099b4fad94dc1714d2bf14c7aedb2e24...     13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submit_16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('s11.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
